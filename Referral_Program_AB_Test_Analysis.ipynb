{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Referral_Program_AB_Test_Analysis\n",
    "## Hao Wu\n",
    "To encourage more referrals, the product team is running an experiment on the iOS app, called “ios_referral_experiment”, that tests adding new links to the referral invite page. The experiment has 3 groups: the “control” group has no new link; the “tab_only” group has a new navigation tab that links to the page; and the “tab_settings” group has both a new navigation tab and a new link in the settings.  What are the results of this experiment, and what would you recommend to the team?\n",
    "\n",
    "### Description of the data\n",
    "Included with this exercise is a set of csv files containing data for the following tables.\n",
    "### split_test_exposures\n",
    "This table includes all occurrences when a user is exposed to an experiment.\n",
    "- user_id : unique id for the user\n",
    "- exposed_time : time when the user was exposed to the experiment\n",
    "- split_test_name : name of the split test the user was exposed to\n",
    "- split_test_group : the split test group the user was assigned to\n",
    "### events\n",
    "This table includes click and view events in the UI.\n",
    "- user_id : unique id for the user\n",
    "- event_time : time when the event occurred\n",
    "- event_type : the type of event that happened\n",
    "- “ referral_page_view ”: user views the page for sending referral invites\n",
    "- “ referrer_page_invite_action ”: users clicks the button to send a referral invite\n",
    "- event_type_button : the type of button the user clicks for the event\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_rows', 500)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "matplotlib.rc('figure', figsize = (14, 7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Data process and validation\n",
    "\n",
    "In this section, I will check the user test split and events datasets. The check will cover:\n",
    "1. Basic data structure and missing values;\n",
    "\n",
    "2. Update the variable type and process missing valuues;\n",
    "\n",
    "3. For user test split dataset, I will also check how users were assigned to the experiment groups.\n",
    "\n",
    "#### 1.1 User test split table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check data types\n",
      "user_id             object\n",
      "split_test_name     object\n",
      "split_test_group    object\n",
      "exposed_time        object\n",
      "dtype: object\n",
      "\n",
      "\n",
      "Check missing values\n",
      "user_id             False\n",
      "split_test_name     False\n",
      "split_test_group    False\n",
      "exposed_time        False\n",
      "dtype: bool\n",
      "\n",
      "\n",
      "Print examples\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>split_test_name</th>\n",
       "      <th>split_test_group</th>\n",
       "      <th>exposed_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>650be962eaef1648482b228babb28973</td>\n",
       "      <td>ios_referral_experiment</td>\n",
       "      <td>control</td>\n",
       "      <td>2017-06-29 18:37:01.890000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2c35e245e4f8bd07cd4239ee79cc3aed</td>\n",
       "      <td>ios_referral_experiment</td>\n",
       "      <td>tab_settings</td>\n",
       "      <td>2017-06-30 04:38:12.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bbb4ec386f36f3b1a6ccbaf8d656e940</td>\n",
       "      <td>ios_referral_experiment</td>\n",
       "      <td>tab_settings</td>\n",
       "      <td>2017-06-30 04:47:27.386000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            user_id          split_test_name split_test_group  \\\n",
       "0  650be962eaef1648482b228babb28973  ios_referral_experiment          control   \n",
       "1  2c35e245e4f8bd07cd4239ee79cc3aed  ios_referral_experiment     tab_settings   \n",
       "2  bbb4ec386f36f3b1a6ccbaf8d656e940  ios_referral_experiment     tab_settings   \n",
       "\n",
       "                 exposed_time  \n",
       "0  2017-06-29 18:37:01.890000  \n",
       "1  2017-06-30 04:38:12.475000  \n",
       "2  2017-06-30 04:47:27.386000  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the split test exposres data \n",
    "split_test_exposures=pd.read_csv(\"./Copy of Coinbase Product Analytics Exercise Data/split_test_exposures.csv\")\n",
    "print(\"Check data types\")\n",
    "print(split_test_exposures.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"Check missing values\")\n",
    "print(split_test_exposures.isnull().any())\n",
    "print(\"\\n\")\n",
    "print(\"Print examples\")\n",
    "split_test_exposures.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>split_test_name</th>\n",
       "      <th>split_test_group</th>\n",
       "      <th>exposed_time</th>\n",
       "      <th>exposed_date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>187670</td>\n",
       "      <td>187670</td>\n",
       "      <td>187670</td>\n",
       "      <td>187670</td>\n",
       "      <td>187670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>182456</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>187656</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>c982505bb972a13c9b6aed6d2dca8896</td>\n",
       "      <td>ios_referral_experiment</td>\n",
       "      <td>tab_only</td>\n",
       "      <td>2017-07-04 15:07:18.504000</td>\n",
       "      <td>2017-07-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>2</td>\n",
       "      <td>187670</td>\n",
       "      <td>68424</td>\n",
       "      <td>2</td>\n",
       "      <td>18533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>first</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-06-29 00:49:58.621000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>last</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-09-01 23:13:36.352000</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 user_id          split_test_name  \\\n",
       "count                             187670                   187670   \n",
       "unique                            182456                        1   \n",
       "top     c982505bb972a13c9b6aed6d2dca8896  ios_referral_experiment   \n",
       "freq                                   2                   187670   \n",
       "first                                NaN                      NaN   \n",
       "last                                 NaN                      NaN   \n",
       "\n",
       "       split_test_group                exposed_time exposed_date  \n",
       "count            187670                      187670       187670  \n",
       "unique                3                      187656           65  \n",
       "top            tab_only  2017-07-04 15:07:18.504000   2017-07-05  \n",
       "freq              68424                           2        18533  \n",
       "first               NaN  2017-06-29 00:49:58.621000          NaN  \n",
       "last                NaN  2017-09-01 23:13:36.352000          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert the time variable to datetime format, and extract date to a new variable\n",
    "split_test_exposures[\"exposed_time\"]=pd.to_datetime(split_test_exposures[\"exposed_time\"])\n",
    "split_test_exposures[\"exposed_date\"]=split_test_exposures[\"exposed_time\"].dt.date\n",
    "split_test_exposures.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SpecificationError",
     "evalue": "nested renamer is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mSpecificationError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-ea9790c02ddb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#check how experiment groups were split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m split_test_exposures.groupby(\"split_test_group\")[\"exposed_time\"].agg({\"user_count\":\"count\",\"start_time\":\"min\",                                                      \n\u001b[0;32m----> 3\u001b[0;31m                                                                       \"end_date\":\"max\"})\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36maggregate\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m             \u001b[0;31m# but not the class list / tuple itself.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_maybe_mangle_lambdas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_aggregate_multiple_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mrelabeling\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mret\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/pandas/core/groupby/generic.py\u001b[0m in \u001b[0;36m_aggregate_multiple_funcs\u001b[0;34m(self, arg)\u001b[0m\n\u001b[1;32m    292\u001b[0m             \u001b[0;31m# GH 15931\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selected_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 294\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mSpecificationError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nested renamer is not supported\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mSpecificationError\u001b[0m: nested renamer is not supported"
     ]
    }
   ],
   "source": [
    "#check how experiment groups were split\n",
    "split_test_exposures.groupby(\"split_test_group\")[\"exposed_time\"].agg({\"user_count\":\"count\",\"start_time\":\"min\",                                                      \n",
    "                                                                      \"end_date\":\"max\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Key takeaways**:\n",
    "\n",
    "- First, Based on the summary of the data, users started getting exposed to the experiment from 6/29/2019 through 9/1/2019. The table is not unique at user_id level, and there are 5214(~3%) users showing up at least two times. In this experiment, the unit of diversion is supposed to be a user, because it makes sense to keep the user experience consistent.\n",
    "\n",
    "- Second, users in \"tab_only\" are more than the other two groups by ~15%. After 7/27/2017, no user was exposed to the \"control\" or \"tab_setting\" groups, but \"tab_only\" group had users exposed over 7/28/2017 to 9/1/2019. This will introduce bias in the performance.\n",
    "\n",
    "**Next Step**\n",
    "\n",
    "I will figure out if the duplicate users were assigned into different groups. Or they were assigned in the identical group but the system marked them twice due to technical issues. Will also look into the users who were exposed to \"tab_only\" groups after 07/27/2019. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the split table is unique at the user_id and split_test_group level\n",
    "len(split_test_exposures)==len(split_test_exposures.drop_duplicates(subset=[\"user_id\",\"split_test_group\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The split test table is unique at the user_id and split_test_group, which means for some users, they were assigned to different groups. Next step, I will first mark those users, and check the experiment groups were split with and without them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a key to mark users have multiple assignments\n",
    "cnt_user_msk=split_test_exposures.groupby([\"user_id\"]).cumcount()+1\n",
    "dup_users=split_test_exposures.loc[cnt_user_msk>1,\"user_id\"].unique()\n",
    "len(dup_users)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#how the second group of dupelicated users were assigned\n",
    "split_test_exposures.loc[cnt_user_msk>=2].groupby(\"split_test_group\")[\"exposed_time\"].agg({\"user_count\":\"count\",\n",
    "                                                                                           \"start_time\":\"min\",\n",
    "                                                                                           \"end_date\":\"max\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the double-assigned users got exposed to the second group which is \"tab_only\" from 07/27/2019. It might be caused from the technical issue. It's safe to drop all of them to avoid introducing biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check users who were assigned tab_only after 07/27/2019 but not double assigned\n",
    "time_msk=split_test_exposures.exposed_time>pd.Timestamp(\"2017-07-28\")\n",
    "dupe_msk=split_test_exposures.user_id.isin(dup_users)\n",
    "print(sum(time_msk&~dupe_msk), \"\\t\" ,sum(time_msk&~dupe_msk)/len(split_test_exposures))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3719 users (~2%) users assigned after 7/27/2017. To avoid introducing factors that likely impact on the test results, those users have to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop users who were assigned into two different groups and were assigned to \"tab_only\" after 7/27/2019\n",
    "split_test_exposures.loc[~time_msk&~dupe_msk].groupby(\"split_test_group\")[\"exposed_time\"].agg({\"user_count\":\"count\",\n",
    "                                                                                    \"start_time\":\"min\",\n",
    "                                                                                    \"end_date\":\"max\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#percent of data kept \n",
    "user_split=split_test_exposures.loc[~time_msk&~dupe_msk]\n",
    "len(user_split)/len(split_test_exposures)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**~8% users are dropped from the analysis. Although the % difference between the largest group, tab_only, and tab_settings is ~4%, it's acceptable in the experiment setting.**\n",
    "\n",
    "#### 1.2 Events table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the events table\n",
    "events=pd.read_csv(\"./Copy of Coinbase Product Analytics Exercise Data/events.csv\")\n",
    "print(\"Check data types\")\n",
    "print(events.dtypes)\n",
    "print(\"\\n\")\n",
    "print(\"Check missing values\")\n",
    "print(events.isnull().any())\n",
    "print(\"Print examples\")\n",
    "events.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are missing values in \"event_type_button\", will check it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the time variable to datetime format, and extract date to a new variable\n",
    "events[\"event_time\"]=pd.to_datetime(events[\"event_time\"])\n",
    "events[\"event_date\"]=pd.to_datetime(events[\"event_time\"]).dt.date\n",
    "events.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the values in the \"event_type_button\"\n",
    "events.groupby(\"event_type\")[\"event_type_button\"].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable \"event_type_button\" was attributed to \"NaN\" when \"event_type\" is \"referrer_page_viewed\". No extra process is neccesary. Need convert all values in \"event_type_button\" to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "events[\"event_type_button\"]=events[\"event_type_button\"].str.lower()\n",
    "print(events[\"event_type_button\"].value_counts())\n",
    "print(\"\\n\")\n",
    "#split data into ios and web events\n",
    "ios_events=events.loc[events[\"platform\"]==\"iOS\"]\n",
    "web_events=events.loc[events[\"platform\"]==\"Web\"]\n",
    "print(ios_events.shape)\n",
    "print(web_events.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 Define and calculate relavant metrics and evaluate the results\n",
    "\n",
    "To measure the impacts from this new feature, following metrics wil be calculated:\n",
    "\n",
    "- **Click probablity:**  percent of users who viewed referrer page, calculated by count of users who visit referrer page devided by total user count in the group. For control group, the percent of users visiting referrer page will be calculated. \n",
    "\n",
    " \n",
    "- **Invite send rate:**     percent of users who send out at least one invite, calculated by count of users send out at lease one invite devided by total user count in the group.t For the control group, the percent of users sending out invites will be calculated. \n",
    "\n",
    " \n",
    "- **Invite to click ratio:** ratio of count of invites to count of clicks, calculated by total invite count devided by total click count.\n",
    "\n",
    "The referree sign-up table is provided, but there's no direct link to map the referrees and referrers to the \"invite sent\" events. Therefore if the invites bring new users will not be evaluated. \n",
    "\n",
    "**To compare the test groups against control group, the two-sample prortion tests will be applied.** The p values from the tests, the differences ($p_{test}-p_{control}$), and confidence interval of differences will be calulated. SInce two treats are in the experiments, the significant level is adjusted to 97.5% ($1-\\frac{\\alpha}{2}, \\alpha=0.05$), i.e. test will be treated as significant if **p value<0.025.**\n",
    "\n",
    "**The overall metrics** and test results will be calculated. Since users started getting assigned to the experiment in different dates,  **metrics will also be calculated and tested by cohort** which is devided by the exposure days of experiment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge the user split table and event table to flag the users in the event table \n",
    "event_exp_join=ios_events.merge(user_split,left_on=[\"user_id\"],\n",
    "             right_on=[\"user_id\"],how=\"inner\")\n",
    "print(len(event_exp_join),\"\\t\", len(event_exp_join)/len(ios_events))\n",
    "#calculate the days between experiment exposed date and event date\n",
    "event_exp_join[\"exposure_days\"]=(event_exp_join.event_date-event_exp_join.exposed_date).astype('timedelta64[D]')\n",
    "#keep the events happening after users exposed to the expetiment\n",
    "event_exp_join=event_exp_join.loc[event_exp_join[\"exposure_days\"]>=0]\n",
    "print(len(event_exp_join),\"\\t\", len(event_exp_join)/len(ios_events))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define a ictionary, including names of metrics as keys, and list of denominators and numeritors as values\n",
    "metric_dict={\"click_thru_prob\":[\"user_count\",\"click_thru_user_count\"],\n",
    "             \"invite_send_rate\":[\"user_count\",\"referrer_user_count\"],\n",
    "             \"invite_click_rate\":[\"click_count\",\"referrer_invite_count\"]}\n",
    "\n",
    "#define help functions to calculate the metrics, and apply the proportion test between the treatment groups against the control group\n",
    "def mectric_cal(user_df,event_df,level):\n",
    "    '''\n",
    "    This function will calculate the metrics, output the metrics as requested level.\n",
    "    Arguments:\n",
    "    user_df: dataFrame user count at specific level\n",
    "    even_df: dataFrameevent table including the user group and exposure day information\n",
    "    level: string or list indicating the level metrics will be aggregated to\n",
    "    Returns:\n",
    "    smry -- dataFrame metrics of three groups at speficit level\n",
    "   \n",
    "    '''\n",
    "    #calculate the click probability \n",
    "    smry=user_df.join(event_df.loc[event_df[\"event_type\"]==\"referrer_page_viewed\"].groupby(level)[\"user_id\"].\\\n",
    "                     agg({\"click_thru_user_count\":\"nunique\",\n",
    "                           \"click_count\":\"count\"}))\n",
    "   \n",
    "    smry[\"click_thru_prob\"]=smry[\"click_thru_user_count\"]/smry[\"user_count\"]\n",
    "    \n",
    "    #calculate the invite_send_rate and ratio of invite count to click count\n",
    "    smry=smry.join(event_df.loc[event_df[\"event_type\"]==\"referrer_page_invite_action\"].\\\n",
    "                           groupby(level)[\"user_id\"].agg({\"referrer_user_count\":\"nunique\",\n",
    "                                                          \"referrer_invite_count\":\"count\"}))\n",
    "    smry[\"click_thru_user_count\"].fillna(0,inplace=True)\n",
    "    smry[\"referrer_user_count\"].fillna(0,inplace=True)\n",
    "    smry[\"referrer_invite_count\"].fillna(0,inplace=True)\n",
    "    smry[\"invite_send_rate\"]=smry[\"referrer_user_count\"]/smry[\"user_count\"]\n",
    "    \n",
    "    smry[\"invite_click_rate\"]=smry[\"referrer_invite_count\"]/smry[\"click_count\"]                                                                       \n",
    "       \n",
    "    return smry\n",
    "\n",
    "def test_result(df,metric_dict=metric_dict):\n",
    "    '''\n",
    "    This function caculate the p values of proportion test, differences, and confidence interval of differences.\n",
    "    Arguments:\n",
    "    df: dataFrame metric table\n",
    "    metric_dict-- Dictionary, including names of metrics as keys, and list of denominators and numeritors as values\n",
    " \n",
    "    Returns:\n",
    "    result -- dataFrame include all statistical values. \n",
    "    \n",
    "    '''\n",
    "    #import packages\n",
    "    from statsmodels.stats.proportion import proportions_ztest\n",
    "    import scipy.stats as st\n",
    "    #define result table\n",
    "    result=pd.DataFrame(columns=[\"metric\",\"split_test_group\",\"difference\",\"p_value\",\"CI_low\",\"CI_upp\"])\n",
    "    \n",
    "    for metric in metric_dict.keys():\n",
    "        counts=metric_dict[metric]\n",
    "        for group in [\"tab_only\",\"tab_settings\"]:\n",
    "            msk=df.index.isin([\"control\",group])\n",
    "        #get the sample size\n",
    "            nobs=   np.array(df.loc[msk,counts[0]])\n",
    "        #get the counts from the table\n",
    "            ncounts=np.array(df.loc[msk,counts[1]])\n",
    "        #calculate p value and zstats\n",
    "            zstats, p_ = proportions_ztest(ncounts, nobs)\n",
    "            thru_rate=np.array(df.loc[msk,metric])\n",
    "            #calculate difference between treatment and control\n",
    "            diff_rate=thru_rate[1]-thru_rate[0]\n",
    "            #calculate the s.e. via z statistics and multiply by 97.5% critical value from normal distribution\n",
    "            m=abs(diff_rate/zstats)*st.norm.ppf(.975)\n",
    "            result=result.append({\"metric\":metric,\n",
    "                                  \"split_test_group\":group,\n",
    "                                  \"difference\":diff_rate,\n",
    "                                   \"p_value\":p_,\n",
    "                                   \"CI_low\":diff_rate-m,\n",
    "                                   \"CI_upp\":diff_rate+m},ignore_index=True)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Check overall metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#caculate the overall metrics\n",
    "all_user_df=user_split.groupby(\"split_test_group\")[\"user_id\"].agg({\"user_count\":\"nunique\"})\n",
    "overall=mectric_cal(all_user_df,event_exp_join,\"split_test_group\")\n",
    "overall[metric_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion test results between test groups and control group\n",
    "test_result(overall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of overall metrics\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 6))\n",
    "for i in range(3):\n",
    "    metric=list(metric_dict.keys())[i]\n",
    "    overall.plot(y=metric,ax=ax[i],kind=\"bar\")\n",
    "    ax[i].set_title(metric, fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "- Overall, the new-added link brought more users to the referrer page, and “tab_only” (difference ~9,9%) had better performance than the “tab_settings”( difference 8.2%). \n",
    "- There are significant increases of percent of users sending out invites in the two treatment groups. \n",
    "- All tests are significant at 97.5% level.\n",
    "- In terms of the ratios of invites to clicks, the test groups are very close to the control group. The confidence intervals of differences are very close 0, and “tab_only” group includes the 0.\n",
    "\n",
    "**Next step:**\n",
    "Check the metrics calculated by exposure days of experiment, to observe how the metrics change over time. The proportion test comparing test groups against control group will be applied as well. \n",
    "\n",
    "### 2.2 Check Metrics by Cohort\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create user counts by exposure days and split group\n",
    "user_split[\"max_exposure_day\"]=(max(event_exp_join.event_date)-user_split[\"exposed_date\"]).astype('timedelta64[D]')\n",
    "temp=user_split.groupby([\"max_exposure_day\",\"split_test_group\"],as_index=False)[\"user_id\"].agg({\"user_count\":\"nunique\"})                                          \n",
    "user_cohort_df=pd.DataFrame()\n",
    "for i in range(65):\n",
    "    grp=temp.loc[temp[\"max_exposure_day\"]>=i].\\\n",
    "    groupby(\"split_test_group\",as_index=False).agg({\"user_count\":\"sum\"})\n",
    "    grp[\"exposure_days\"]=i\n",
    "    user_cohort_df=user_cohort_df.append(grp)\n",
    "del temp\n",
    "user_cohort_df.set_index([\"exposure_days\",\"split_test_group\"],inplace=True) \n",
    "\n",
    "#Calculate the cohort metrics\n",
    "cohort=mectric_cal(user_cohort_df,event_exp_join,[\"exposure_days\",\"split_test_group\"])\n",
    "cohort[metric_dict.keys()].head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of metrics over exposure days\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(14, 21))\n",
    "for i in range(3):\n",
    "    metric=list(metric_dict.keys())[i]\n",
    "    cohort.unstack().plot(y=metric,ax=ax[i])\n",
    "    ax[i].set_title(metric, fontsize=20)\n",
    "    ax[i].set_ylabel('percent')\n",
    "    cohort[metric_dict.keys()]\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#perform proportion test for each cohort\n",
    "cohort_test=cohort.reset_index(level=\"exposure_days\").groupby(\"exposure_days\").apply(test_result)\n",
    "cohort_test.reset_index(\"exposure_days\",inplace=True)\n",
    "cohort_test.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the p values above 0.025. \n",
    "p_msk=cohort_test.p_value>0.025\n",
    "cohort_test.loc[p_msk,].groupby([\"metric\",\"split_test_group\"]).exposure_days.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if the differences and confidence intervals of difference close to 0\n",
    "ci_msk=(np.sign(cohort_test.CI_upp) != np.sign(cohort_test.CI_low)) | (cohort_test.difference<=0)\n",
    "cohort_test.loc[ci_msk,].groupby([\"metric\",\"split_test_group\"]).exposure_days.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualization of differenes over exposure days\n",
    "fig, ax = plt.subplots(nrows=3, ncols=1, figsize=(12, 21))\n",
    "for i in range(3):\n",
    "    metric=list(metric_dict.keys())[i]\n",
    "    for grp,df in cohort_test.groupby(\"split_test_group\"):    \n",
    "        y=df.loc[df.metric==metric,\"difference\"]\n",
    "        x=df.loc[df.metric==metric,\"exposure_days\"]\n",
    "        ax[i].plot(x,y,label=grp)\n",
    "    ax[i].set_title(metric+\"_differences\", fontsize=20)\n",
    "    ax[i].axhline(y=0,color=\"red\",ls=\"--\")\n",
    "    ax[i].legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Findings:**\n",
    "\n",
    "Observe how the metrics change over time.\n",
    "\n",
    "- The click_thru_prob and invite_send_rate increase dramatically in the first three days.\n",
    "\n",
    "- After 10 days, both test groups still win, but they are getting closer and closer.\n",
    "\n",
    "- After 25 days, the differences of click_thru_prob between test groups and the control group are not significant. Although the “tab_only” group win over the control most time, the differences are very small. The invite_send_rate are fluctuating around 0, and no significant winner after 25 days.\n",
    "\n",
    "- For the invite click rates, they change dramatically over day, no pattern can be identified.\n",
    "\n",
    "**The proportion tests validate the findings.**\n",
    "\n",
    "- Tests of click-through probabilities are significantly higher in the first 25 days and 28 days for tab_only and tab_setting groups respectively;\n",
    "\n",
    "- Click sent rate are significantly higher in the first 16 and 17 days for tab_only and tab_setting groups.\n",
    "\n",
    "- Differences of invite click rates are not significant in most exposure days.\n",
    "\n",
    "\n",
    "### Part 3 Conclusion and recommendation\n",
    "\n",
    "Overall, the new-added link of the referrer page is helpful bringing more users to visit the page, and leading more users to send out invites. However, as the test goes on, the differences between the tests and control groups are getting closer. After 30 days we don’t see any significant differences. \n",
    "\n",
    "The temporary lift in metrics partly is caused by the novelty effect. The new link brings a lot of attention and leads users to review the page and send out invites. With time, the lift disappears because the change is no longer novelty. \n",
    "\n",
    "Another reason to explain the lift will not last in the long term is,  the count of the referral invites from one user is decreasing over time. Users have limited numbers of potential referees, and it will take long time to make new friends and get more new referees. If users have invited all potential referees, they will not land the referrer page in a long time. \n",
    "\n",
    "If in the short term, users bring more new users via the referral program, it’s still a win for the company in the long term. Therefore I would recommend adding the the navigation tab that links to the page to the iOS platform.\n",
    "\n",
    "Based on the AB test results:\n",
    "\n",
    "- Overall, \"tab_only\" group had slightly higher proportions of users visiting the referrer page and sending invites than the \"tab_setting\" group. \n",
    "\n",
    "- By cohort, in the click through probablity, the differences between the \"tab_only\" group and control group are slightly above 0 after 30 days. \n",
    "\n",
    "Even though the performance differences between the two test groups are very minor, the \"tab_only\" still is a better feature to roll out. \n",
    "\n",
    "In the meantime, some **education prompt and referral program campaign** should be added to remind current users and educate new uers. For example, pop-up app banners or campaign emails. \n",
    "\n",
    "In conclusion, to encourage more referrals in the long term, adding the tab to direct users to the referrer page and using periodical promp or campaigns get more attentions will be helpful and practical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
